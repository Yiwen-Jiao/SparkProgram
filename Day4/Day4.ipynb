{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "CASSANDRA_IP=os.getenv('CASSANDRA1')\n",
    "print(CASSANDRA_IP)\n",
    "\n",
    "if CASSANDRA_IP is None:\n",
    "    CASSANDRA_IP = '172.18.0.2'\n",
    "\n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster([CASSANDRA_IP])\n",
    "session = cluster.connect()\n",
    "session.execute('DROP KEYSPACE IF EXISTS classroom')\n",
    "session.execute(\"CREATE KEYSPACE classroom WITH REPLICATION={'class':'SimpleStrategy', 'replication_factor':'1'}\")\n",
    "session = cluster.connect('classroom')\n",
    "session.execute(\"create table student(id int PRIMARY KEY, firstname text, lastname text, emails set<text>)\")\n",
    "session.execute(\"insert into student (id, firstname, lastname, emails) values (1, 'Joe', 'Smith', {'joes@xyz.com', 'joe.smith@abc.net'})\")\n",
    "session.execute(\"update student set firstname = 'Joseph' where id = 1\")\n",
    "session.execute(\"insert into student (id, firstname, lastname, emails) values (2, 'Mike', 'Jones', {'mikej@xyz.com', 'mike.jones@def.net', 'mike1234@gmail.com'})\")\n",
    "rows = session.execute('SELECT id, firstname, lastname, emails from student')\n",
    "print(list(rows))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order for Spark to talk to Cassandra it needs to know the IP address to initialize the spark context with and it also needs the spark-cassandra-connector.\n",
    "\n",
    "### Mongo will be similar and we need to initial the spark context with pointers to the mongo uri and also include the mongo-spark-connector\n",
    "\n",
    "### Additionally, whoever configures the cluster may need to make sure additional jars are installed in $SPARK_HOME/jars\n",
    "\n",
    "Don't run the following, it is just included here to have a look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.1,com.datastax.spark:spark-cassandra-connector_2.11:2.4.0 pyspark-shell'\n",
    "\n",
    "def initspark(appname = \"Test\", servername = \"local\", cassandra=\"127.0.0.1\", mongo=\"mongodb://127.0.0.1/classroom\"):\n",
    "    print ('initializing pyspark')\n",
    "    conf = SparkConf().set(\"spark.cassandra.connection.host\", cassandra).setAppName(appname).setMaster(servername)\n",
    "    sc = SparkContext(conf=conf)\n",
    "    spark = SparkSession.builder.appName(appname) \\\n",
    "    .config(\"spark.mongodb.input.uri\", mongo) \\\n",
    "    .config(\"spark.mongodb.output.uri\", mongo) \\\n",
    "    .enableHiveSupport().getOrCreate()\n",
    "    sc.setLogLevel(\"WARN\")\n",
    "    print ('pyspark initialized')\n",
    "    return sc, spark, conf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's initialize as usual but pass in the IP address of the Cassandra cluster this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/class')\n",
    "from initspark import *\n",
    "sc, spark, conf = initspark(cassandra=CASSANDRA_IP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the spark-cassandra-connector we can read from a cassandra table in a similar way to how we read from a MySQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"student\", keyspace=\"classroom\").load()\n",
    "display(people)\n",
    "print(people.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results of a DataFrame can also be written to a Cassandra table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the results of a DataFrame into a Cassandra table\n",
    "x = sc.parallelize([(3, 'Mary', 'Johnson', ['Mary1@gmail.com', 'Mary2@yahoo.com'])])\n",
    "x1 = spark.createDataFrame(x, schema = ['id', 'firstname', 'lastname', 'emails'])\n",
    "x1.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"student\", keyspace=\"classroom\").mode(\"append\").save()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"student\", keyspace=\"classroom\").load()\n",
    "display(people)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once you have a DataFrame pointing to a cassandra table, you can use normal SparkSQL on it by making it a temporary view. Here we will flatten out the list of emails using LATERAL VIEW EXPLODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.createOrReplaceTempView('people')\n",
    "people2 = spark.sql('select id, firstname, lastname, email from people LATERAL VIEW EXPLODE(emails) EXPLODED_TABLE AS email')\n",
    "display(people2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatively you could also use dot syntax and make method calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people3 = people2.where(\"email like '%.com'\").orderBy(\"id\")\n",
    "display(people3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python directly talking to Mongo without Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "client = pymongo.MongoClient(\"mongodb://127.0.0.1:27017/\")\n",
    "classroom = client[\"classroom\"]\n",
    "if 'classroom' in (x['name'] for x in client.list_databases()):\n",
    "    client.drop_database('classroom')\n",
    "\n",
    "people = classroom['people']\n",
    "name = {\"firstname\" : \"Adam\", \"personid\":4}\n",
    "x = people.insert_one(name)\n",
    "\n",
    "names = [{\"firstname\" : \"Betty\", \"personid\":5}\n",
    "         ,{\"firstname\" : \"Charlie\", \"personid\":6}]\n",
    "x = people.insert_many(names)\n",
    "\n",
    "x = people.find()\n",
    "print ('*' * 80)\n",
    "print ('from mongo directly')\n",
    "print (list(x))\n",
    "print ('*' * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/classroom.people\").load()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Like Cassandra before, we can take a DataFrame and write it to a Mongo destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([(7, 'David')])\n",
    "x1 = spark.createDataFrame(x, schema = ['personid', 'firstname'])\n",
    "x1.write.format(\"mongo\").options(collection=\"people\", database=\"classroom\").mode(\"append\").save()\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/classroom.people\").load()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Like any DataFrame, we can make it into a temporary view and use SparkSQL on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('people')\n",
    "spark.sql('select * from people').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
