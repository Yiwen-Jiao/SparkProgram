{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Day4.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwtJNbsUmTry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "CASSANDRA_IP=os.getenv('CASSANDRA1')\n",
        "print(CASSANDRA_IP)\n",
        "\n",
        "if CASSANDRA_IP is None:\n",
        "    CASSANDRA_IP = '172.18.0.2'\n",
        "\n",
        "from cassandra.cluster import Cluster\n",
        "cluster = Cluster([CASSANDRA_IP])\n",
        "session = cluster.connect()\n",
        "session.execute('DROP KEYSPACE IF EXISTS classroom')\n",
        "session.execute(\"CREATE KEYSPACE classroom WITH REPLICATION={'class':'SimpleStrategy', 'replication_factor':'1'}\")\n",
        "session = cluster.connect('classroom')\n",
        "session.execute(\"create table student(id int PRIMARY KEY, firstname text, lastname text, emails set<text>)\")\n",
        "session.execute(\"insert into student (id, firstname, lastname, emails) values (1, 'Joe', 'Smith', {'joes@xyz.com', 'joe.smith@abc.net'})\")\n",
        "session.execute(\"update student set firstname = 'Joseph' where id = 1\")\n",
        "session.execute(\"insert into student (id, firstname, lastname, emails) values (2, 'Mike', 'Jones', {'mikej@xyz.com', 'mike.jones@def.net', 'mike1234@gmail.com'})\")\n",
        "rows = session.execute('SELECT id, firstname, lastname, emails from student')\n",
        "print(list(rows))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyznNkl6mTr7",
        "colab_type": "text"
      },
      "source": [
        "### In order for Spark to talk to Cassandra, it needs to know the IP address to initialize the spark context with and it also needs the spark-cassandra-connector.\n",
        "\n",
        "### Mongo will be similar and we need to initial the spark context with pointers to the mongo uri and also include the mongo-spark-connector.\n",
        "\n",
        "### Additionally, whoever configures the cluster may need to make sure additional jars are installed in $SPARK_HOME/jars\n",
        "\n",
        "Don't run the following, it is just included here to have a look at."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM10f8dimTr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.1,com.datastax.spark:spark-cassandra-connector_2.11:2.4.0 pyspark-shell'\n",
        "\n",
        "def initspark(appname = \"Test\", servername = \"local\", cassandra=\"127.0.0.1\", mongo=\"mongodb://127.0.0.1/classroom\"):\n",
        "    print ('initializing pyspark')\n",
        "    conf = SparkConf().set(\"spark.cassandra.connection.host\", cassandra).setAppName(appname).setMaster(servername)\n",
        "    sc = SparkContext(conf=conf)\n",
        "    spark = SparkSession.builder.appName(appname) \\\n",
        "    .config(\"spark.mongodb.input.uri\", mongo) \\\n",
        "    .config(\"spark.mongodb.output.uri\", mongo) \\\n",
        "    .enableHiveSupport().getOrCreate()\n",
        "    sc.setLogLevel(\"WARN\")\n",
        "    print ('pyspark initialized')\n",
        "    return sc, spark, conf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIwStPDumTsB",
        "colab_type": "text"
      },
      "source": [
        "### Let's initialize as usual but pass in the IP address of the Cassandra cluster this time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdmgZyjXmTsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/class')\n",
        "from initspark import *\n",
        "sc, spark, conf = initspark(cassandra=CASSANDRA_IP)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feH3XJspmTsH",
        "colab_type": "text"
      },
      "source": [
        "### Using the spark-cassandra-connector, we can read from a Cassandra table in a similar way to how we read from a MySQL table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UnZdz6PmTsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "people = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"student\", keyspace=\"classroom\").load()\n",
        "display(people)\n",
        "print(people.collect())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bbld9o82mTsM",
        "colab_type": "text"
      },
      "source": [
        "### The results of a DataFrame can also be written to a Cassandra table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whh4IIf2mTsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Append the results of a DataFrame into a Cassandra table\n",
        "x = sc.parallelize([(3, 'Mary', 'Johnson', ['Mary1@gmail.com', 'Mary2@yahoo.com'])])\n",
        "x1 = spark.createDataFrame(x, schema = ['id', 'firstname', 'lastname', 'emails'])\n",
        "x1.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"student\", keyspace=\"classroom\").mode(\"append\").save()\n",
        "print('Done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o12NKO7MmTsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "people = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"student\", keyspace=\"classroom\").load()\n",
        "display(people)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shplLy9UmTsT",
        "colab_type": "text"
      },
      "source": [
        "## LAB: ## \n",
        "### Read the shippers file from /class/datasets/northwind/JSON and save it to a Cassandra table in the classroom keyspace. Switch to a cqlsh window to prove it worked.\n",
        "<br>\n",
        "<details><summary>Click for <b>hint</b></summary>\n",
        "<p>\n",
        "Use printSchema to see the schema of the Spark DataFrame\n",
        "<br>\n",
        "The Cassandra table must exist before you can write to it, string in Spark is text in Cassandra, long is bigint in Cassandra\n",
        "<br>\n",
        "Build a create table command and executed it on the Cassandra session object\n",
        "<br>\n",
        "Write the DataFrame into the newly created table\n",
        "<br>\n",
        "<br>\n",
        "</p>\n",
        "</details>\n",
        "\n",
        "<details><summary>Click for <b>code</b></summary>\n",
        "<p>\n",
        "\n",
        "```python\n",
        "shippers = spark.read.json('/class/datasets/northwind/JSON/shippers')\n",
        "shippers.printSchema()\n",
        "session.execute(\"create table if not exists shippers(companyname text, phone text, shipperid bigint PRIMARY KEY)\")\n",
        "shippers.write.format(\"org.apache.spark.sql.cassandra\").options(table=\"shippers\", keyspace=\"classroom\").mode(\"append\").save()\n",
        "\n",
        "```\n",
        "</p>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCaBh7OVmTsW",
        "colab_type": "text"
      },
      "source": [
        "shippers = spark.read.json('/class/datasets/northwind/JSON/shippers')\n",
        "shippers.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pDFrulBmTsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NywplNKmTsa",
        "colab_type": "text"
      },
      "source": [
        "### Once you have a DataFrame pointing to a Cassandra table, you can use normal SparkSQL on it by making it a temporary view. Here we will flatten out the list of emails using LATERAL VIEW EXPLODE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHc9I00rmTsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "people.createOrReplaceTempView('people')\n",
        "people2 = spark.sql('select id, firstname, lastname, email from people LATERAL VIEW EXPLODE(emails) EXPLODED_TABLE AS email')\n",
        "display(people2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ULhznuamTsf",
        "colab_type": "text"
      },
      "source": [
        "### Alternatively, you could also use dot syntax and make method calls."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd8Hx49xmTsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "people3 = people2.where(\"email like '%.com'\").orderBy(\"id\")\n",
        "display(people3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARKnO4DbmTsk",
        "colab_type": "text"
      },
      "source": [
        "### Python directly talking to Mongo without Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5StpouLmTsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pymongo\n",
        "client = pymongo.MongoClient(\"mongodb://127.0.0.1:27017/\")\n",
        "classroom = client[\"classroom\"]\n",
        "if 'classroom' in (x['name'] for x in client.list_databases()):\n",
        "    client.drop_database('classroom')\n",
        "\n",
        "people = classroom['people']\n",
        "name = {\"firstname\" : \"Adam\", \"personid\":4}\n",
        "x = people.insert_one(name)\n",
        "\n",
        "names = [{\"firstname\" : \"Betty\", \"personid\":5}\n",
        "         ,{\"firstname\" : \"Charlie\", \"personid\":6}]\n",
        "x = people.insert_many(names)\n",
        "\n",
        "x = people.find()\n",
        "print ('*' * 80)\n",
        "print ('from mongo directly')\n",
        "print (list(x))\n",
        "print ('*' * 80)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOjVg5_OmTsq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/classroom.people\").load()\n",
        "df.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otXXWlKcmTst",
        "colab_type": "text"
      },
      "source": [
        "### Like Cassandra before, we can take a DataFrame and write it to a Mongo destination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4Qf4tu9mTsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = sc.parallelize([(7, 'David')])\n",
        "x1 = spark.createDataFrame(x, schema = ['personid', 'firstname'])\n",
        "x1.write.format(\"mongo\").options(collection=\"people\", database=\"classroom\").mode(\"append\").save()\n",
        "print('Done')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60inzyxbmTsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = spark.read.format(\"mongo\").option(\"uri\", \"mongodb://127.0.0.1/classroom.people\").load()\n",
        "df.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Hs3nGamTs1",
        "colab_type": "text"
      },
      "source": [
        "### Like any DataFrame, we can make it into a temporary view and use SparkSQL on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbLK9SC9mTs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.createOrReplaceTempView('people')\n",
        "spark.sql('select * from people').show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtyt9EvAmTs7",
        "colab_type": "text"
      },
      "source": [
        "## LAB: ## \n",
        "### Write shippers to Mongo and find all the shippers with an 800 number using  a temporary view.\n",
        "<br>\n",
        "<details><summary>Click for <b>hint</b></summary>\n",
        "<p>\n",
        "Unlike Cassandra, Mongo does not require a collection to exist before writing to it, so just write the DataFrame to a new collection\n",
        "<br>\n",
        "Make a DataFrame from the new Mongo collection and turn it into a temporary view\n",
        "<br>\n",
        "Use SQL-like expression to find the desired records\n",
        "<br>\n",
        "<br>\n",
        "</p>\n",
        "</details>\n",
        "\n",
        "<details><summary>Click for <b>code</b></summary>\n",
        "<p>\n",
        "\n",
        "```python\n",
        "shippers.write.format(\"mongo\").options(collection=\"shippers\", database=\"classroom\").mode(\"append\").save()\n",
        "\n",
        "s=spark.read.format(\"mongo\").option(\"uri\",\"mongodb://127.0.0.1/classroom.shippers\").load()\n",
        "s.createOrReplaceTempView('shippers')\n",
        "display(spark.sql(\"select * from shippers where phone like '%800%'\"))\n",
        "```\n",
        "</p>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyAE_C9VmTs8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "innQIsfGmTtB",
        "colab_type": "text"
      },
      "source": [
        "## HOMEWORK: ## \n",
        "**First Challenge**\n",
        "\n",
        "Read Products from any source and write it to a Cassandra table. For simplicity, we only need to keep the productid, productname, and unitprice columns.\n",
        "\n",
        "**Second Challenge**\n",
        "\n",
        "Read Orders_LineItems.json from Day3 folder and write it to a Mongo collection.\n",
        "\n",
        "**Third Challenge**\n",
        "\n",
        "Join the Products and Orders_LineItems and join then, flatten them and regroup them so that the orders are grouped under each product instead.\n",
        "\n",
        "**Bonus Challenge**\n",
        "\n",
        "Include a calculated column showing how many times each product was ordered.\n",
        "\n",
        "A starting template has been provided in Day4-Homework.py to deal with preparing the Cassandra and Mongo environments for Challenges 1 & 2. If you have difficulty doing those on your own, then start with that template. Otherwise, try it from scratch from the code provided in the course so far.\n",
        "<br>\n",
        "<details><summary>Click for <b>hint</b></summary>\n",
        "<p>\n",
        "<br>\n",
        "Read each table from the NoSQL source and turn it into a temporary view\n",
        "<br>\n",
        "Use LATERAL VIEW EXPLODE() EXPLODED_TABLE to flatten out the nested format file or orders\n",
        "<br>\n",
        "Use the flattened results to join to products\n",
        "<br>\n",
        "Use the results of the join to group on productid, productname, and collect a structured list of customerid, orderid, orderdate, productid, quantity and price\n",
        "<br>\n",
        "Use the size function on the collected list to determine how many times each product was ordered or alternatively do it as part of the SQL query with other familiar techniques\n",
        "<br>\n",
        "<br>\n",
        "</p>\n",
        "</details>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmj4i8jEmTtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}