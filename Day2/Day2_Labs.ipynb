{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/student/ROI/SparkProgram')\n",
    "from initspark import *\n",
    "sc, spark, conf = initspark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LAB:** Use the regions and territories RDDs from the previous lab and convert them into  dataframes with meaningful schemas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|RegionID|RegionName|\n",
      "+--------+----------+\n",
      "|       1|   Eastern|\n",
      "|       2|   Western|\n",
      "|       3|  Northern|\n",
      "|       4|  Southern|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regions = sc.textFile('hdfs://localhost:9000/regions')\n",
    "regions = regions.map(lambda x : x.split(',')).map(lambda x : (int(x[0]), x[1]))\n",
    "regionsdf = regions.toDF('RegionID:int, RegionName:string')\n",
    "regionsdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------+\n",
      "|TerritoryID|TerritoryName|RegionID|\n",
      "+-----------+-------------+--------+\n",
      "|       1581|     Westboro|       1|\n",
      "|       1730|      Bedford|       1|\n",
      "|       1833|    Georgetow|       1|\n",
      "|       2116|       Boston|       1|\n",
      "|       2139|    Cambridge|       1|\n",
      "|       2184|    Braintree|       1|\n",
      "|       2903|   Providence|       1|\n",
      "|       3049|       Hollis|       3|\n",
      "|       3801|   Portsmouth|       3|\n",
      "|       6897|       Wilton|       1|\n",
      "|       7960|   Morristown|       1|\n",
      "|       8837|       Edison|       1|\n",
      "|      10019|     New York|       1|\n",
      "|      10038|     New York|       1|\n",
      "|      11747|     Mellvile|       1|\n",
      "|      14450|     Fairport|       1|\n",
      "|      19428| Philadelphia|       3|\n",
      "|      19713|       Neward|       1|\n",
      "|      20852|    Rockville|       1|\n",
      "|      27403|   Greensboro|       1|\n",
      "+-----------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "territories = sc.textFile('hdfs://localhost:9000/territories')\n",
    "territories = territories.map(lambda x : x.split(',')).map(lambda x : (int(x[0]), x[1], int(x[2])))\n",
    "territoriesdf = territories.toDF('TerritoryID:int, TerritoryName:string, RegionID: int')\n",
    "territoriesdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LAB:** Using the df3 dataframe answer the following questions:\n",
    "How many Platinum card purchases where there with a discount above $100\n",
    "Find the ten biggest discount amounts earned by women and show just the purchase amount, discount and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+-------------+--------+\n",
      "|RegionID|RegionName|TerritoryID|TerritoryName|RegionID|\n",
      "+--------+----------+-----------+-------------+--------+\n",
      "|       1|   Eastern|       1581|     Westboro|       1|\n",
      "|       1|   Eastern|       1730|      Bedford|       1|\n",
      "|       1|   Eastern|       1833|    Georgetow|       1|\n",
      "|       1|   Eastern|       2116|       Boston|       1|\n",
      "|       1|   Eastern|       2139|    Cambridge|       1|\n",
      "|       1|   Eastern|       2184|    Braintree|       1|\n",
      "|       1|   Eastern|       2903|   Providence|       1|\n",
      "|       1|   Eastern|       6897|       Wilton|       1|\n",
      "|       1|   Eastern|       7960|   Morristown|       1|\n",
      "|       1|   Eastern|       8837|       Edison|       1|\n",
      "|       1|   Eastern|      10019|     New York|       1|\n",
      "|       1|   Eastern|      10038|     New York|       1|\n",
      "|       1|   Eastern|      11747|     Mellvile|       1|\n",
      "|       1|   Eastern|      14450|     Fairport|       1|\n",
      "|       1|   Eastern|      19713|       Neward|       1|\n",
      "|       1|   Eastern|      20852|    Rockville|       1|\n",
      "|       1|   Eastern|      27403|   Greensboro|       1|\n",
      "|       1|   Eastern|      27511|         Cary|       1|\n",
      "|       1|   Eastern|      40222|   Louisville|       1|\n",
      "|       3|  Northern|       3049|       Hollis|       3|\n",
      "+--------+----------+-----------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f33f9702d619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mregionsdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterritoriesdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregionsdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegionID\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mterritoriesdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegionID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CardType = 'Platinum' and Discount > 100\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gender = 'F'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Amount'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Amount'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Discount'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df3' is not defined"
     ]
    }
   ],
   "source": [
    "print(df3.where(\"CardType = 'Platinum' and Discount > 100\").count())\n",
    "df3.where(\"Gender = 'F'\").orderBy('Amount', ascending = False).select('Amount', 'Discount', 'Date').take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LAB:** Read the Products file from the JSON folder and categories from ths CSVHeaders folder, then join them displaying just the product and category IDs and names, and sort by categoryID then productID. \n",
    "\n",
    "Hint. drop the ambiguous column after the join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- categoryid: long (nullable = true)\n",
      " |-- discontinued: long (nullable = true)\n",
      " |-- productid: long (nullable = true)\n",
      " |-- productname: string (nullable = true)\n",
      " |-- quantityperunit: string (nullable = true)\n",
      " |-- reorderlevel: long (nullable = true)\n",
      " |-- supplierid: long (nullable = true)\n",
      " |-- unitprice: double (nullable = true)\n",
      " |-- unitsinstock: long (nullable = true)\n",
      " |-- unitsonorder: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- CategoryName: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      "\n",
      "+----------+------------+---------+--------------------+\n",
      "|CategoryID|CategoryName|productid|         productname|\n",
      "+----------+------------+---------+--------------------+\n",
      "|         1|   Beverages|        1|                Chai|\n",
      "|         1|   Beverages|        2|               Chang|\n",
      "|         1|   Beverages|       24|  Guarana Fantastica|\n",
      "|         1|   Beverages|       34|       Sasquatch Ale|\n",
      "|         1|   Beverages|       35|      Steeleye Stout|\n",
      "|         1|   Beverages|       38|       Cote de Blaye|\n",
      "|         1|   Beverages|       39|    Chartreuse verte|\n",
      "|         1|   Beverages|       43|         Ipoh Coffee|\n",
      "|         1|   Beverages|       67|Laughing Lumberja...|\n",
      "|         1|   Beverages|       70|       Outback Lager|\n",
      "|         1|   Beverages|       75|Rhonbrau Klosterbier|\n",
      "|         1|   Beverages|       76|        Lakkalikoori|\n",
      "|         2|  Condiments|        3|       Aniseed Syrup|\n",
      "|         2|  Condiments|        4|Chef Anton's Caju...|\n",
      "|         2|  Condiments|        5|Chef Anton's Gumb...|\n",
      "|         2|  Condiments|        6|Grandma's Boysenb...|\n",
      "|         2|  Condiments|        8|Northwoods Cranbe...|\n",
      "|         2|  Condiments|       15|        Genen Shouyu|\n",
      "|         2|  Condiments|       44|        Gula Malacca|\n",
      "|         2|  Condiments|       61|      Sirop d'erable|\n",
      "+----------+------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products = spark.read.json('/home/student/ROI/SparkProgram/datasets/northwind/JSON/products')\n",
    "#products.show()\n",
    "products.printSchema()\n",
    "\n",
    "categories = spark.read.csv('/home/student/ROI/SparkProgram/datasets/northwind/CSVHeaders/categories', header = True, inferSchema = True)\n",
    "#categories.show()\n",
    "categories.printSchema()\n",
    "\n",
    "c = categories\n",
    "p = products\n",
    "c.join(p, c.CategoryID == p.categoryid).drop(p.categoryid).select('CategoryID', 'CategoryName', 'productid', 'productname').orderBy('categoryid', 'productid').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
