{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a terminal window and run the following commands\n",
    "sudo bash\n",
    "start-hadoop\n",
    "cd /home/student/ROI/SparkProgram/Day3\n",
    "./fixhive.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a simple hive table for regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /home/student/ROI/SparkProgram/Day3/regions.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/student/ROI/SparkProgram')\n",
    "from initspark import *\n",
    "sc, spark, conf = initspark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|regionid|regionname|\n",
      "+--------+----------+\n",
      "|       1|   Eastern|\n",
      "|       2|   Western|\n",
      "|       3|  Northern|\n",
      "|       4|  Southern|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regions = spark.sql('select * from regions')\n",
    "regions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[regionid: int, regionname: string]\n"
     ]
    }
   ],
   "source": [
    "print(regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------+\n",
      "|TerritoryID|TerritoryName|RegionID|\n",
      "+-----------+-------------+--------+\n",
      "|      01581|     Westboro|       1|\n",
      "|      01730|      Bedford|       1|\n",
      "|      01833|    Georgetow|       1|\n",
      "|      02116|       Boston|       1|\n",
      "|      02139|    Cambridge|       1|\n",
      "|      02184|    Braintree|       1|\n",
      "|      02903|   Providence|       1|\n",
      "|      03049|       Hollis|       3|\n",
      "|      03801|   Portsmouth|       3|\n",
      "|      06897|       Wilton|       1|\n",
      "|      07960|   Morristown|       1|\n",
      "|      08837|       Edison|       1|\n",
      "|      10019|     New York|       1|\n",
      "|      10038|     New York|       1|\n",
      "|      11747|     Mellvile|       1|\n",
      "|      14450|     Fairport|       1|\n",
      "|      19428| Philadelphia|       3|\n",
      "|      19713|       Neward|       1|\n",
      "|      20852|    Rockville|       1|\n",
      "|      27403|   Greensboro|       1|\n",
      "+-----------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "territories = spark.read.csv('/home/student/ROI/SparkProgram/datasets/northwind/CSVHeaders/territories', header=True)\n",
    "territories.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------+\n",
      "|TerritoryID|TerritoryName|RegionID|\n",
      "+-----------+-------------+--------+\n",
      "|      01581|     Westboro|       1|\n",
      "|      01730|      Bedford|       1|\n",
      "|      01833|    Georgetow|       1|\n",
      "|      02116|       Boston|       1|\n",
      "|      02139|    Cambridge|       1|\n",
      "|      02184|    Braintree|       1|\n",
      "|      02903|   Providence|       1|\n",
      "|      06897|       Wilton|       1|\n",
      "|      07960|   Morristown|       1|\n",
      "|      08837|       Edison|       1|\n",
      "|      10019|     New York|       1|\n",
      "|      10038|     New York|       1|\n",
      "|      11747|     Mellvile|       1|\n",
      "|      14450|     Fairport|       1|\n",
      "|      19713|       Neward|       1|\n",
      "|      20852|    Rockville|       1|\n",
      "|      27403|   Greensboro|       1|\n",
      "|      27511|         Cary|       1|\n",
      "|      40222|   Louisville|       1|\n",
      "+-----------+-------------+--------+\n",
      "\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "territories.createOrReplaceTempView('territories')\n",
    "t1 =spark.sql('select * from territories where regionid = 1')\n",
    "t1.show()\n",
    "print(t1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "territories.write.saveAsTable('Territories2', mode='overwrite')\n",
    "spark.sql('create table Territories3 as select * from territories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+---------------+\n",
      "|regionid|regionname|territoryid|  territoryname|\n",
      "+--------+----------+-----------+---------------+\n",
      "|       1|   Eastern|      01581|       Westboro|\n",
      "|       1|   Eastern|      01730|        Bedford|\n",
      "|       1|   Eastern|      01833|      Georgetow|\n",
      "|       1|   Eastern|      02116|         Boston|\n",
      "|       1|   Eastern|      02139|      Cambridge|\n",
      "|       1|   Eastern|      02184|      Braintree|\n",
      "|       1|   Eastern|      02903|     Providence|\n",
      "|       1|   Eastern|      06897|         Wilton|\n",
      "|       1|   Eastern|      07960|     Morristown|\n",
      "|       1|   Eastern|      08837|         Edison|\n",
      "|       1|   Eastern|      10019|       New York|\n",
      "|       1|   Eastern|      10038|       New York|\n",
      "|       1|   Eastern|      11747|       Mellvile|\n",
      "|       1|   Eastern|      14450|       Fairport|\n",
      "|       1|   Eastern|      19713|         Neward|\n",
      "|       1|   Eastern|      20852|      Rockville|\n",
      "|       1|   Eastern|      27403|     Greensboro|\n",
      "|       1|   Eastern|      27511|           Cary|\n",
      "|       1|   Eastern|      40222|     Louisville|\n",
      "|       2|   Western|      60179|Hoffman Estates|\n",
      "+--------+----------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rt = spark.sql('select r.regionid, r.regionname, t.territoryid, t.territoryname from regions as r join territories as t on r.regionid = t.regionid order by r.regionid, t.territoryid')\n",
    "rt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[categoryid: bigint, categoryname: string, description: string, picture: string]\n",
      "+----------+--------------+--------------------+-------+\n",
      "|categoryid|  categoryname|         description|picture|\n",
      "+----------+--------------+--------------------+-------+\n",
      "|         1|     Beverages|Soft drinks, coff...|   null|\n",
      "|         2|    Condiments|Sweet and savory ...|   null|\n",
      "|         3|   Confections|Desserts, candies...|   null|\n",
      "|         4|Dairy Products|             Cheeses|   null|\n",
      "|         5|Grains/Cereals|Breads, crackers,...|   null|\n",
      "|         6|  Meat/Poultry|      Prepared meats|   null|\n",
      "|         7|       Produce|Dried fruit and b...|   null|\n",
      "|         8|       Seafood|    Seaweed and fish|   null|\n",
      "+----------+--------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories = spark.read.json('/home/student/ROI/SparkProgram/datasets/northwind/JSON/categories')\n",
    "print(categories)\n",
    "categories.show()\n",
    "categories.createOrReplaceTempView('categories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------------+-------+\n",
      "|categoryid|  categoryname|         description|picture|\n",
      "+----------+--------------+--------------------+-------+\n",
      "|         1|     Beverages|Soft drinks, coff...|   null|\n",
      "|         2|    Condiments|Sweet and savory ...|   null|\n",
      "|         3|   Confections|Desserts, candies...|   null|\n",
      "|         4|Dairy Products|             Cheeses|   null|\n",
      "|         5|Grains/Cereals|Breads, crackers,...|   null|\n",
      "|         6|  Meat/Poultry|      Prepared meats|   null|\n",
      "|         7|       Produce|Dried fruit and b...|   null|\n",
      "|         8|       Seafood|    Seaweed and fish|   null|\n",
      "+----------+--------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from categories').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install mysql-connector-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "succes\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "try:\n",
    "    cn = mysql.connector.connect(host='localhost', user='test', password='password')\n",
    "    cursor = cn.cursor()\n",
    "    cursor.execute('create database if not exists northwind')\n",
    "    cn.close()\n",
    "\n",
    "    cn = mysql.connector.connect(host='localhost', user='test', password='password', database='northwind')\n",
    "    cursor = cn.cursor()    \n",
    "    cursor.execute('drop table if exists regions')\n",
    "    cn.close()\n",
    "except:\n",
    "    print('something went wrong')\n",
    "else:\n",
    "    print('success')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions.write.format(\"jdbc\").options(url=\"jdbc:mysql://localhost/northwind\", driver='com.mysql.jdbc.Driver', dbtable='regions', user='test', password = \"password\", mode = \"append\", useSSL = \"false\").save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|regionid|regionname|\n",
      "+--------+----------+\n",
      "|       1|   Eastern|\n",
      "|       2|   Western|\n",
      "|       3|  Northern|\n",
      "|       4|  Southern|\n",
      "|       5|New Region|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regions2 = spark.read.format(\"jdbc\").options(url=\"jdbc:mysql://localhost/northwind\", driver=\"com.mysql.jdbc.Driver\", dbtable= \"regions\", user=\"test\", password=\"password\").load()\n",
    "regions2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(count(1)=4)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(spark.sql('select count(*) from regions').collect())\n",
    "spark.sql('select * from regions').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------+\n",
      "|TerritoryID|TerritoryName|RegionID|\n",
      "+-----------+-------------+--------+\n",
      "|      01581|     WESTBORO|       1|\n",
      "|      01730|      BEDFORD|       1|\n",
      "|      01833|    GEORGETOW|       1|\n",
      "|      02116|       BOSTON|       1|\n",
      "|      02139|    CAMBRIDGE|       1|\n",
      "|      02184|    BRAINTREE|       1|\n",
      "|      02903|   PROVIDENCE|       1|\n",
      "|      03049|       HOLLIS|       3|\n",
      "|      03801|   PORTSMOUTH|       3|\n",
      "|      06897|       WILTON|       1|\n",
      "|      07960|   MORRISTOWN|       1|\n",
      "|      08837|       EDISON|       1|\n",
      "|      10019|     NEW YORK|       1|\n",
      "|      10038|     NEW YORK|       1|\n",
      "|      11747|     MELLVILE|       1|\n",
      "|      14450|     FAIRPORT|       1|\n",
      "|      19428| PHILADELPHIA|       3|\n",
      "|      19713|       NEWARD|       1|\n",
      "|      20852|    ROCKVILLE|       1|\n",
      "|      27403|   GREENSBORO|       1|\n",
      "+-----------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t1 = spark.sql('select TerritoryID, UPPER(TerritoryName) as TerritoryName, RegionID from territories')\n",
    "t1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TerritoryID: string (nullable = true)\n",
      " |-- TerritoryName: string (nullable = true)\n",
      " |-- RegionID: string (nullable = true)\n",
      "\n",
      "+-----------+-------------+--------+---------+\n",
      "|TerritoryID|TerritoryName|RegionID|upperName|\n",
      "+-----------+-------------+--------+---------+\n",
      "|      01581|     Westboro|       1| WESTBORO|\n",
      "|      01730|      Bedford|       1|  BEDFORD|\n",
      "|      01833|    Georgetow|       1|GEORGETOW|\n",
      "|      02116|       Boston|       1|   BOSTON|\n",
      "|      02139|    Cambridge|       1|CAMBRIDGE|\n",
      "+-----------+-------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------+-------------+--------+---------+---------+\n",
      "|TerritoryID|TerritoryName|RegionID|upperName|titleName|\n",
      "+-----------+-------------+--------+---------+---------+\n",
      "|      01581|     Westboro|       1| WESTBORO| Westboro|\n",
      "|      01730|      Bedford|       1|  BEDFORD|  Bedford|\n",
      "|      01833|    Georgetow|       1|GEORGETOW|Georgetow|\n",
      "|      02116|       Boston|       1|   BOSTON|   Boston|\n",
      "|      02139|    Cambridge|       1|CAMBRIDGE|Cambridge|\n",
      "+-----------+-------------+--------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "t2 = spark.sql('select * from territories')\n",
    "t2.printSchema()\n",
    "#t2.show()\n",
    "t2 = t2.withColumn('upperName', expr('UPPER(TerritoryName)'))\n",
    "t2.show(5)\n",
    "\n",
    "t2 = t2.withColumn('titleName', udf(lambda x : x.title(), StringType())(t2.upperName))\n",
    "t2.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------+------------+\n",
      "|TerritoryID|TerritoryName|RegionID|    Reversed|\n",
      "+-----------+-------------+--------+------------+\n",
      "|      90405| Santa Monica|       2|acinoM atnaS|\n",
      "|      29202|     Columbia|       4|    aibmuloC|\n",
      "|      19428| Philadelphia|       3|aihpledalihP|\n",
      "|      33607|        Tampa|       4|       apmaT|\n",
      "|      95054|  Santa Clara|       2| aralC atnaS|\n",
      "|      30346|      Atlanta|       4|     atnaltA|\n",
      "|      48075|   Southfield|       3|  dleifhtuoS|\n",
      "|      98052|      Redmond|       2|     dnomdeR|\n",
      "|      44122|    Beachwood|       3|   doowhcaeB|\n",
      "|      19713|       Neward|       1|      draweN|\n",
      "|      01730|      Bedford|       1|     drofdeB|\n",
      "|      02903|   Providence|       1|  ecnedivorP|\n",
      "|      02184|    Braintree|       1|   eertniarB|\n",
      "|      02139|    Cambridge|       1|   egdirbmaC|\n",
      "|      85251|   Scottsdale|       2|  eladsttocS|\n",
      "|      11747|     Mellvile|       1|    elivlleM|\n",
      "|      55113|    Roseville|       3|   ellivesoR|\n",
      "|      20852|    Rockville|       1|   ellivkcoR|\n",
      "|      72716|  Bentonville|       4| ellivnotneB|\n",
      "|      40222|   Louisville|       1|  ellivsiuoL|\n",
      "+-----------+-------------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def reverse(x):\n",
    "    return x[::-1]\n",
    "\n",
    "spark.udf.register('reverse', reverse, StringType())\n",
    "\n",
    "spark.sql('select *, reverse(TerritoryName) as Reversed from Territories').orderBy('Reversed').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|RegionID|       TerritoryList|\n",
      "+--------+--------------------+\n",
      "|       3|[Bloomfield Hills...|\n",
      "|       1|[Mellvile, George...|\n",
      "|       4|[Austin, Columbia...|\n",
      "|       2|[Scottsdale, Sant...|\n",
      "+--------+--------------------+\n",
      "\n",
      "root\n",
      " |-- RegionID: string (nullable = true)\n",
      " |-- TerritoryList: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "[Row(RegionID='3', TerritoryList=['Bloomfield Hills', 'Troy', 'Portsmouth', 'Hollis', 'Racine', 'Findlay', 'Southfield', 'Roseville', 'Minneapolis', 'Beachwood', 'Philadelphia'])]\n"
     ]
    }
   ],
   "source": [
    "tr1 = spark.sql(\"SELECT RegionID, collect_set(TerritoryName) AS TerritoryList FROM Territories GROUP BY RegionID\")\n",
    "\n",
    "tr1.show()\n",
    "tr1.printSchema()\n",
    "print(tr1.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RegionID: integer (nullable = true)\n",
      " |-- RegionName: string (nullable = true)\n",
      " |-- TerritoryList: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- TerritoryID: string (nullable = true)\n",
      " |    |    |-- TerritoryName: string (nullable = true)\n",
      "\n",
      "+--------+----------+--------------------+\n",
      "|RegionID|RegionName|       TerritoryList|\n",
      "+--------+----------+--------------------+\n",
      "|       1|   Eastern|[[27511, Cary], [...|\n",
      "|       2|   Western|[[60179, Hoffman ...|\n",
      "|       3|  Northern|[[45839, Findlay]...|\n",
      "|       4|  Southern|[[75234, Dallas],...|\n",
      "+--------+----------+--------------------+\n",
      "\n",
      "[Row(RegionID=1, RegionName='Eastern', TerritoryList=[Row(TerritoryID='27511', TerritoryName='Cary'), Row(TerritoryID='07960', TerritoryName='Morristown'), Row(TerritoryID='02184', TerritoryName='Braintree'), Row(TerritoryID='11747', TerritoryName='Mellvile'), Row(TerritoryID='01581', TerritoryName='Westboro'), Row(TerritoryID='14450', TerritoryName='Fairport'), Row(TerritoryID='20852', TerritoryName='Rockville'), Row(TerritoryID='10038', TerritoryName='New York'), Row(TerritoryID='02903', TerritoryName='Providence'), Row(TerritoryID='02116', TerritoryName='Boston'), Row(TerritoryID='06897', TerritoryName='Wilton'), Row(TerritoryID='19713', TerritoryName='Neward'), Row(TerritoryID='40222', TerritoryName='Louisville'), Row(TerritoryID='27403', TerritoryName='Greensboro'), Row(TerritoryID='08837', TerritoryName='Edison'), Row(TerritoryID='01833', TerritoryName='Georgetow'), Row(TerritoryID='01730', TerritoryName='Bedford'), Row(TerritoryID='10019', TerritoryName='New York'), Row(TerritoryID='02139', TerritoryName='Cambridge')]), Row(RegionID=2, RegionName='Western', TerritoryList=[Row(TerritoryID='60179', TerritoryName='Hoffman Estates'), Row(TerritoryID='94025', TerritoryName='Menlo Park'), Row(TerritoryID='95054', TerritoryName='Santa Clara'), Row(TerritoryID='98104', TerritoryName='Seattle'), Row(TerritoryID='90405', TerritoryName='Santa Monica'), Row(TerritoryID='98004', TerritoryName='Bellevue'), Row(TerritoryID='85014', TerritoryName='Phoenix'), Row(TerritoryID='95060', TerritoryName='Santa Cruz'), Row(TerritoryID='98052', TerritoryName='Redmond'), Row(TerritoryID='60601', TerritoryName='Chicago'), Row(TerritoryID='94105', TerritoryName='San Francisco'), Row(TerritoryID='80909', TerritoryName='Colorado Springs'), Row(TerritoryID='95008', TerritoryName='Campbell'), Row(TerritoryID='80202', TerritoryName='Denver'), Row(TerritoryID='85251', TerritoryName='Scottsdale')])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sql = \"\"\"SELECT r.RegionID, r.RegionName\n",
    ", COLLECT_SET(NAMED_STRUCT(\"TerritoryID\", TerritoryID, \"TerritoryName\", TerritoryName)) AS TerritoryList\n",
    "FROM Regions AS r\n",
    "JOIN Territories AS t ON r.RegionID = t.RegionID\n",
    "GROUP BY r.RegionID, r.RegionName\n",
    "ORDER BY r.RegionID\"\"\"\n",
    "\n",
    "#tr2 = spark.sql(\"SELECT RegionID, COLLECT_SET(NAMED_STRUCT('TerritoryID', TerritoryID, 'TerritoryName', TerritoryName)) AS TerritoryList FROM Territories GROUP BY RegionID\")\n",
    "tr2 = spark.sql(sql)\n",
    "tr2.printSchema()\n",
    "tr2.show()\n",
    "print(tr2.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|RegionID|   TerritoryName|\n",
      "+--------+----------------+\n",
      "|       3|Bloomfield Hills|\n",
      "|       3|            Troy|\n",
      "|       3|      Portsmouth|\n",
      "|       3|          Hollis|\n",
      "|       3|          Racine|\n",
      "|       3|         Findlay|\n",
      "|       3|      Southfield|\n",
      "|       3|       Roseville|\n",
      "|       3|     Minneapolis|\n",
      "|       3|       Beachwood|\n",
      "|       3|    Philadelphia|\n",
      "|       1|        Mellvile|\n",
      "|       1|       Georgetow|\n",
      "|       1|        Fairport|\n",
      "|       1|       Cambridge|\n",
      "|       1|          Neward|\n",
      "|       1|            Cary|\n",
      "|       1|          Boston|\n",
      "|       1|       Rockville|\n",
      "|       1|        Westboro|\n",
      "+--------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr1.createOrReplaceTempView('RegionTerritories')\n",
    "sql = \"\"\"SELECT RegionID, TerritoryName\n",
    "FROM RegionTerritories\n",
    "LATERAL VIEW EXPLODE(TerritoryList) EXPLODED_TABLE AS TerritoryName\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+--------------------+\n",
      "|RegionId|RegionName|               Terr1|               Terr2|\n",
      "+--------+----------+--------------------+--------------------+\n",
      "|       1|   Eastern|       [27511, Cary]|  [02184, Braintree]|\n",
      "|       2|   Western|[60179, Hoffman E...|[95054, Santa Clara]|\n",
      "|       3|  Northern|    [45839, Findlay]|[55439, Minneapolis]|\n",
      "|       4|  Southern|     [75234, Dallas]|    [30346, Atlanta]|\n",
      "+--------+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr2.createOrReplaceTempView('RegionTerritories')\n",
    "spark.sql(\"select RegionId, RegionName, TerritoryList[0] as Terr1, TerritoryList[2] as Terr2 from RegionTerritories\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+---------------+\n",
      "|RegionID|RegionName|TerritoryID|  TerritoryName|\n",
      "+--------+----------+-----------+---------------+\n",
      "|       1|   Eastern|      27511|           Cary|\n",
      "|       1|   Eastern|      07960|     Morristown|\n",
      "|       1|   Eastern|      02184|      Braintree|\n",
      "|       1|   Eastern|      11747|       Mellvile|\n",
      "|       1|   Eastern|      01581|       Westboro|\n",
      "|       1|   Eastern|      14450|       Fairport|\n",
      "|       1|   Eastern|      20852|      Rockville|\n",
      "|       1|   Eastern|      10038|       New York|\n",
      "|       1|   Eastern|      02903|     Providence|\n",
      "|       1|   Eastern|      02116|         Boston|\n",
      "|       1|   Eastern|      06897|         Wilton|\n",
      "|       1|   Eastern|      19713|         Neward|\n",
      "|       1|   Eastern|      40222|     Louisville|\n",
      "|       1|   Eastern|      27403|     Greensboro|\n",
      "|       1|   Eastern|      08837|         Edison|\n",
      "|       1|   Eastern|      01833|      Georgetow|\n",
      "|       1|   Eastern|      01730|        Bedford|\n",
      "|       1|   Eastern|      10019|       New York|\n",
      "|       1|   Eastern|      02139|      Cambridge|\n",
      "|       2|   Western|      60179|Hoffman Estates|\n",
      "+--------+----------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"SELECT RegionID, RegionName, Territory.TerritoryID AS TerritoryID\n",
    ", Territory.TerritoryName AS TerritoryName\n",
    "FROM RegionTerritories\n",
    "LATERAL VIEW EXPLODE(TerritoryList) EXPLODED_TABLE AS Territory\n",
    "\"\"\"\n",
    "spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
