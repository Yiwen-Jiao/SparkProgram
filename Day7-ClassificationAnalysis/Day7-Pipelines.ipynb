{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing pyspark\n",
      "pyspark initialized\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "rootpath = '/class'\n",
    "datapath = f'{rootpath}/datasets/'\n",
    "sys.path.append(rootpath)\n",
    "import pyspark_helpers as pyh\n",
    "from pyspark_helpers import *\n",
    "sc, spark, conf = initspark()\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib as mp\n",
    "import numpy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pyspark_helpers import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following helper function shows the building of stages to convert categorical and numeric columns into Vectorized versions using a Pipeline instead of building the steps as a series of DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeMLPipeline(df, categorical_features, numeric_features, target_label = None, target_is_categorical = True):\n",
    "    from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler, StringIndexerModel\n",
    "    from pyspark.ml import Pipeline\n",
    "\n",
    "    stages = []\n",
    "\n",
    "    for c in categorical_features:\n",
    "        stringIndexer = StringIndexer(inputCol = c, outputCol = c + '_Index')\n",
    "        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[c + \"_classVec\"])\n",
    "        stages += [stringIndexer, encoder]\n",
    "        \n",
    "    if target_is_categorical:\n",
    "        label_stringIdx = StringIndexer(inputCol = target_label, outputCol = 'label')\n",
    "        stages += [label_stringIdx]\n",
    "\n",
    "    assemblerInputs = [c + \"_classVec\" for c in categorical_features] + numeric_features\n",
    "    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "    stages += [assembler]\n",
    "\n",
    "    pipeline = Pipeline(stages = stages)\n",
    "\n",
    "    return pipeline\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the bank data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>deposit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2343</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>1042</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>45</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>1467</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>1270</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>1389</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2476</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>579</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>184</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>673</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42</td>\n",
       "      <td>management</td>\n",
       "      <td>single</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>562</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>830</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>6</td>\n",
       "      <td>may</td>\n",
       "      <td>1201</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60</td>\n",
       "      <td>retired</td>\n",
       "      <td>divorced</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>545</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>6</td>\n",
       "      <td>may</td>\n",
       "      <td>1030</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>37</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>6</td>\n",
       "      <td>may</td>\n",
       "      <td>608</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>28</td>\n",
       "      <td>services</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>5090</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>6</td>\n",
       "      <td>may</td>\n",
       "      <td>1297</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         job   marital  education default  balance housing loan  \\\n",
       "0   59      admin.   married  secondary      no     2343     yes   no   \n",
       "1   56      admin.   married  secondary      no       45      no   no   \n",
       "2   41  technician   married  secondary      no     1270     yes   no   \n",
       "3   55    services   married  secondary      no     2476     yes   no   \n",
       "4   54      admin.   married   tertiary      no      184      no   no   \n",
       "5   42  management    single   tertiary      no        0     yes  yes   \n",
       "6   56  management   married   tertiary      no      830     yes  yes   \n",
       "7   60     retired  divorced  secondary      no      545     yes   no   \n",
       "8   37  technician   married  secondary      no        1     yes   no   \n",
       "9   28    services    single  secondary      no     5090     yes   no   \n",
       "\n",
       "   contact  day month  duration  campaign  pdays  previous poutcome deposit  \n",
       "0  unknown    5   may      1042         1     -1         0  unknown     yes  \n",
       "1  unknown    5   may      1467         1     -1         0  unknown     yes  \n",
       "2  unknown    5   may      1389         1     -1         0  unknown     yes  \n",
       "3  unknown    5   may       579         1     -1         0  unknown     yes  \n",
       "4  unknown    5   may       673         2     -1         0  unknown     yes  \n",
       "5  unknown    5   may       562         2     -1         0  unknown     yes  \n",
       "6  unknown    6   may      1201         1     -1         0  unknown     yes  \n",
       "7  unknown    6   may      1030         1     -1         0  unknown     yes  \n",
       "8  unknown    6   may       608         1     -1         0  unknown     yes  \n",
       "9  unknown    6   may      1297         3     -1         0  unknown     yes  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename = 'bank.csv'\n",
    "df = spark.read.csv(f'{datapath}/finance/{filename}', header = True, inferSchema = True)\n",
    "display(df)\n",
    "\n",
    "# Save a pointer to the raw data\n",
    "dfRawFile = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer()\n",
    "\n",
    "col = 'marital'\n",
    "m_indexer = StringIndexer(inputCol = col, outputCol = col+'_Index')\n",
    "x1 = m_indexer.fit(df).transform(df) #.select(col, col+'_Index')\n",
    "\n",
    "m_encoder = OneHotEncoderEstimator(inputCols=[col + '_Index'], outputCols=[col+'_Vector'])\n",
    "x2 = m_encoder.fit(x1).transform(x1).orderBy(col + '_Index')\n",
    "\n",
    "col = 'job'\n",
    "j_indexer = StringIndexer(inputCol = col, outputCol = col+'_Index')\n",
    "x3 = j_indexer.fit(x2).transform(x2)\n",
    "j_encoder = OneHotEncoderEstimator(inputCols=[col + '_Index'], outputCols=[col+'_Vector'])\n",
    "x4 = j_encoder.fit(x3).transform(x3)\n",
    "\n",
    "#display(x2.select('marital', 'marital_Index', 'marital_Vector'))\n",
    "end = timer()\n",
    "print('time to run', end - start)\n",
    "display(x2)\n",
    "\n",
    "start = timer()\n",
    "col = 'marital'\n",
    "m_indexer = StringIndexer(inputCol = col, outputCol = col+'_Index')\n",
    "m_encoder = OneHotEncoderEstimator(inputCols=[col + '_Index'], outputCols=[col+'_Vector'])\n",
    "#pipeline = Pipeline(stages = [m_indexer, m_encoder])\n",
    "\n",
    "col = 'job'\n",
    "j_indexer = StringIndexer(inputCol = col, outputCol = col+'_Index')\n",
    "j_encoder = OneHotEncoderEstimator(inputCols=[col + '_Index'], outputCols=[col+'_Vector'])\n",
    "\n",
    "v_encoder = VectorAssembler(inputCols = ['age','marital_Vector', 'job_Vector'], outputCol = 'features')\n",
    "pipeline = Pipeline(stages = [m_indexer, j_indexer, m_encoder, j_encoder, v_encoder])\n",
    "dfModel = pipeline.fit(df)\n",
    "#dfModel.save()\n",
    "dfML = dfModel.transform(df)\n",
    "end = timer()\n",
    "print('time to run', end - start)\n",
    "display(dfML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "col = 'marital'\n",
    "indexer = StringIndexer(inputCol = col, outputCol = col+'_Index')\n",
    "x1 = indexer.fit(df).transform(df) #.select(col, col+'_Index')\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols=[col + '_Index'], outputCols=[col+'_Vector'])\n",
    "x2 = encoder.fit(x1).transform(x1).orderBy(col + '_Index')\n",
    "display(x2.select('marital', 'marital_Index', 'marital_Vector'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the same categorical and numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just keep a few fields to start with for simplicity\n",
    "numeric_features = ['age','balance', 'duration', 'pdays']\n",
    "categorical_features = ['job', 'marital', 'education', 'housing', 'loan', 'contact', 'campaign', 'poutcome', 'deposit']\n",
    "\n",
    "# numeric_features = ['balance', 'duration', 'age']\n",
    "# categorical_features = ['marital', 'education']\n",
    "target_label = 'default'\n",
    "\n",
    "\n",
    "df = dfRawFile.select(numeric_features + categorical_features + [target_label])\n",
    "display(df)\n",
    "print(df.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try this using the original helper vs the Pipeline version to see if there is a time difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer()\n",
    "\n",
    "dfModel, dfML = MakeMLDataFramePipeline(df, categorical_features, numeric_features, target_label)\n",
    "#dfML = pyh.MakeMLDataFramePipeline(df, categorical_features, numeric_features, target_label)\n",
    "#dfML = pyh.MakeMLDataFrame(df, categorical_features, numeric_features, target_label)\n",
    "\n",
    "display(dfML)\n",
    "dfML.printSchema()\n",
    "labelCnt = dfML.groupBy('label').count()\n",
    "display(labelCnt)\n",
    "\n",
    "end = timer()\n",
    "print('time to run', end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test as normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = dfML.randomSplit([.7,.3], seed = 10)\n",
    "print (f'Training set row count {train.count()}')\n",
    "print (f'Testing set row count {test.count()}')\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "dtModel = dt.fit(train)\n",
    "print('DT Trained')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtPredictions, dtLog = pyh.predict_and_evaluate(dtModel, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict = dict(age=59, balance=2343, duration=1042, pdays=-1, job='admin.', marital='married', education='secondary', housing='yes', loan='no', contact='unknown', campaign=1, poutcome='unknown', deposit='yes')\n",
    "print(predict)\n",
    "predict = spark.createDataFrame(sc.parallelize([predict]))\n",
    "print(predict)\n",
    "predictML = dfModel.transform(predict)\n",
    "#x = dtModel.transform(predict)\n",
    "\n",
    "print(predictML.take(1))\n",
    "\n",
    "prediction = dtModel.transform(predictML).select('prediction')\n",
    "print(prediction.collect()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bankdefault(transformModel, predictionModel, d): #age, balance, duration, pdays, job, marital, education, housing, loan, contact, campaign, poutcome, deposit):\n",
    "    newDF = spark.createDataFrame(sc.parallelize([d]))\n",
    "    predictML = transformModel.transform(newDF)\n",
    "    prediction = predictionModel.transform(predictML)\n",
    "    return (prediction.collect())[0][0]\n",
    "\n",
    "predict = dict(age=19, balance=2343, duration=1042, pdays=-1, job='admin.', marital='married', education='secondary', housing='yes', loan='no', contact='unknown', campaign=1, poutcome='unknown', deposit='yes')\n",
    "print (predict_bankdefault(dfModel, dtModel, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines and writing your own models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Estimator, Model, Pipeline, Transformer\n",
    "from pyspark.ml.param.shared import *\n",
    "from pyspark.sql.functions import avg, stddev_samp\n",
    "\n",
    "\n",
    "class myTransformer(Transformer):\n",
    "    pass\n",
    "\n",
    "class HasMean(Params):\n",
    "\n",
    "    mean = Param(Params._dummy(), \"mean\", \"mean\", \n",
    "        typeConverter=TypeConverters.toFloat)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasMean, self).__init__()\n",
    "\n",
    "    def setMean(self, value):\n",
    "        return self._set(mean=value)\n",
    "\n",
    "    def getMean(self):\n",
    "        return self.getOrDefault(self.mean)\n",
    "    \n",
    "class HasStandardDeviation(Params):\n",
    "\n",
    "    stddev = Param(Params._dummy(), \"stddev\", \"stddev\", \n",
    "        typeConverter=TypeConverters.toFloat)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasStandardDeviation, self).__init__()\n",
    "\n",
    "    def setStddev(self, value):\n",
    "        return self._set(stddev=value)\n",
    "\n",
    "    def getStddev(self):\n",
    "        return self.getOrDefault(self.stddev)\n",
    "\n",
    "class HasCenteredThreshold(Params):\n",
    "\n",
    "    centered_threshold = Param(Params._dummy(),\n",
    "            \"centered_threshold\", \"centered_threshold\",\n",
    "            typeConverter=TypeConverters.toFloat)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasCenteredThreshold, self).__init__()\n",
    "\n",
    "    def setCenteredThreshold(self, value):\n",
    "        return self._set(centered_threshold=value)\n",
    "\n",
    "    def getCenteredThreshold(self):\n",
    "        return self.getOrDefault(self.centered_threshold)\n",
    "    \n",
    "class NormalDeviation(Estimator, HasInputCol, \n",
    "        HasPredictionCol, HasCenteredThreshold):\n",
    "\n",
    "    def _fit(self, dataset):\n",
    "        c = self.getInputCol()\n",
    "        mu, sigma = dataset.agg(avg(c), stddev_samp(c)).first()\n",
    "        return (NormalDeviationModel()\n",
    "            .setInputCol(c)\n",
    "            .setMean(mu)\n",
    "            .setStddev(sigma)\n",
    "            .setCenteredThreshold(self.getCenteredThreshold())\n",
    "            .setPredictionCol(self.getPredictionCol()))\n",
    "\n",
    "class NormalDeviationModel(Model, HasInputCol, HasPredictionCol,\n",
    "        HasMean, HasStandardDeviation, HasCenteredThreshold):\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        x = self.getInputCol()\n",
    "        y = self.getPredictionCol()\n",
    "        threshold = self.getCenteredThreshold()\n",
    "        mu = self.getMean()\n",
    "        sigma = self.getStddev()\n",
    "\n",
    "        return dataset.withColumn(y, (dataset[x] - mu) > threshold * sigma)\n",
    "\n",
    "df = sc.parallelize([(1, 2.0), (2, 3.0), (3, 0.0), (4, 99.0)]).toDF([\"id\", \"x\"])\n",
    "\n",
    "normal_deviation = NormalDeviation().setInputCol(\"x\").setCenteredThreshold(1.0)\n",
    "model  = Pipeline(stages=[normal_deviation]).fit(df)\n",
    "#print(normal_deviation.getMean())\n",
    "model.transform(df).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PandasUDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pd.DataFrame(pd.Series(range(11)), columns=[\"x\"]))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbl(x):\n",
    "    return x * 2\n",
    "\n",
    "nums1 = [1, 2, 3, 4]\n",
    "nums2 = []\n",
    "for n in nums1:\n",
    "    nums2.append(dbl(n)) \n",
    "\n",
    "print (nums1 * 2)\n",
    "\n",
    "import numpy as np\n",
    "nums3 = np.array([1, 2, 3, 4])\n",
    "nums4 = nums3 * 2\n",
    "\n",
    "print (nums3 * 2)\n",
    "\n",
    "\n",
    "print(nums3 - nums3.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def func1(x):\n",
    "    return x + 1\n",
    "\n",
    "display(df.withColumn('func1', udf(func1, 'int')(df.x)))\n",
    "\n",
    "func1x = udf(func1, 'int')\n",
    "display(df.withColumn('func1x', func1x(df.x)))\n",
    "\n",
    "sqr = udf(lambda x : x * x , 'int') \n",
    "display(df.withColumn('x3', sqr(df.x)))\n",
    "\n",
    "@udf('int')\n",
    "def square(x):\n",
    "      return x * 2\n",
    "\n",
    "display(df.withColumn('x2', square(df.x)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pyarrow\n",
    "#import pyarrow\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "#from pyspark.sql.types import LongType\n",
    "\n",
    "@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "def psquare(x):\n",
    "      return x * x\n",
    "   \n",
    "#pandas_square = pandas_udf(psquare, returnType=LongType())\n",
    "\n",
    "df.withColumn('x3', psquare(df.x)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
